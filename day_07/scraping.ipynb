{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  HTML\n",
    "### (Hyper Text Markup Language)\n",
    "    - 링크로 연결된 텍스트 페이지. 자유롭게 이동이 가능하다.\n",
    "      웹페이지를 구성 (html:뼈대/css:옷/js:특수기능)\n",
    "      \n",
    "      실습: test.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1=스크레이핑이란?\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#분석하고 싶은 HTML\n",
    "\n",
    "html='''\n",
    "<html><body>\n",
    "    <h1>스크레이핑이란?</h1>\n",
    "    <p>웹 페이지를 분석하는 것</p>\n",
    "    <p>원하는 부분을 추출하는 것</p>\n",
    "</body></html>\n",
    "'''\n",
    "#HTML 분석하기\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "#원하는 부분 선택 출력\n",
    "h1 = soup.html.body.h1\n",
    "\n",
    "#선택된요소에서 글자 추출\n",
    "print(\"h1=\"+h1.string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1=html web page ㅎㅎ\n",
      "p1=힝힝\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#분석하고 싶은 HTML\n",
    "\n",
    "html=\"/Users/bomilee/Documents/GitHub/python_basic/day_07/test.html\"\n",
    "\n",
    "\n",
    "#HTML 분석하기\n",
    "soup = BeautifulSoup(open(html), 'html.parser')\n",
    "\n",
    "#원하는 부분 선택 출력\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "\n",
    "#선택된요소에서 글자 추출\n",
    "print(\"h1=\"+h1.string)\n",
    "print(\"p1=\"+p1.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1=html web page ㅎㅎ\n",
      "p1=힝힝\n",
      "p2=힝힝2\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#분석하고 싶은 HTML\n",
    "\n",
    "html=\"/Users/bomilee/Documents/GitHub/python_basic/day_07/test.html\"\n",
    "\n",
    "\n",
    "#HTML 분석하기\n",
    "soup = BeautifulSoup(open(html), 'html.parser')\n",
    "\n",
    "#원하는 부분 선택 출력\n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "p2 = p1.next_sibling.next_sibling\n",
    "\n",
    "#선택된요소에서 글자 추출\n",
    "print(\"h1=\"+h1.string)\n",
    "print(\"p1=\"+p1.string)\n",
    "print(\"p2=\"+p2.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "참고사이트  \n",
    "https://hyanghope.tistory.com/322  \n",
    "https://hyanghope.tistory.com/469?category=962942  \n",
    "https://m.blog.naver.com/PostView.nhn?blogId=imsam77&logNo=221258961328&proxyReferer=https%3A%2F%2Fwww.google.com%2F  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title= 나는 타이틀이다.. 하 하 하 \n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#분석하고 싶은 HTML\n",
    "html=\"/Users/bomilee/Documents/GitHub/python_basic/day_07/test.html\"\n",
    "\n",
    "#HTML 분석하기\n",
    "soup = BeautifulSoup(open(html), 'html.parser')\n",
    "\n",
    "#원하는 부분 선택 출력\n",
    "title = soup.find(id=\"title\")\n",
    "\n",
    "#선택된요소에서 글자 추출\n",
    "print(\"title=\"+title.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naver > https://www.naver.com\n",
      "daum > https://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#분석하고 싶은 HTML\n",
    "html='''\n",
    "<html><body>\n",
    "    <ul>\n",
    "        <li><a href='https://www.naver.com'>naver</a></li>\n",
    "        <li><a href='https://www.daum.net'>daum</a></li>\n",
    "    </ul>\n",
    "</body></html>\n",
    "'''\n",
    "\n",
    "#HTML 분석하기\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "links = soup.find_all(\"a\")\n",
    "\n",
    "for a in links:\n",
    "    href = a.attrs['href']\n",
    "    text = a.string\n",
    "    print(text,\">\",href)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n",
      "기압골과 동풍의 영향으로 10월 1일과 2일은 강원영동과 남부지방, 제주도에 비가 오겠고, 강원영동과 경상도는 3일까지 이어지는 곳이 있겠습니다.<br />한편, 동풍의 영향으로 5일 강원영동에는 비가 오겠습니다. 그 밖의 날은 고기압의 가장자리에 들어 구름많겠습니다.<br />기온은 평년(최저기온: 7~17℃, 최고기온: 21~25℃)보다 조금 높겠습니다.<br />강수량은 평년(1~6mm)보다 중부지방(강원영동 제외)은 적겠으나, 강원영동과 남부지방, 제주도는 많겠습니다.<br /><br />* 괌 서쪽해상에 위치한 열대저압부의 발달과 이동경로에 따라 10월 1일 이후의 예보 변동성이 크겠으니, 앞으로 발표되는 예보와 기상정보를 참고하기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "url = \"http://www.weather.go.kr/weather/forecast/mid-term-rss3.jsp\"\n",
    "\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "title = soup.find(\"title\").string\n",
    "wf = soup.find(\"wf\").string\n",
    "print(title)\n",
    "print(wf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1= 영화\n",
      "li= 스타트랙\n",
      "li= 화려한휴가\n",
      "li= 토이스토리\n",
      "li= 어바웃타임\n",
      "li= 신비한동물사전\n",
      "li= 괴물\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html='''\n",
    "<html><body>\n",
    "    <div id=\"movie\">\n",
    "        <h1>영화</h1>\n",
    "        <ul class=\"items\">\n",
    "            <li>스타트랙</li>\n",
    "            <li>화려한휴가</li>\n",
    "            <li>토이스토리</li>\n",
    "            <li>어바웃타임</li>\n",
    "            <li>신비한동물사전</li>\n",
    "            <li>괴물</li>\n",
    "        </ul>\n",
    "    \n",
    "    </div>\n",
    "</body></html>\n",
    "'''\n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "h1 = soup.select_one(\"div#movie > h1\").string\n",
    "print(\"h1=\",h1)\n",
    "\n",
    "li_list = soup.select(\"div#movie > ul.items > li\")\n",
    "for li in li_list:\n",
    "    print(\"li=\",li.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "달러 =  1,200.00원입니다\n",
      "엔화 =  1,110.03원입니다\n",
      "유로화 =  1,312.44원입니다\n",
      "위엔화 168.51원입니다\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "#ssl error ㅠㅠ 힝..\n",
    "#https://stackoverflow.com/questions/27835619/urllib-and-ssl-certificate-verify-failed-error\n",
    "\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "price = soup.select(\"span.value\")\n",
    "\n",
    "print(\"달러 = \", price[0].string + \"원입니다\")\n",
    "print(\"엔화 = \", price[1].string + \"원입니다\")\n",
    "print(\"유로화 = \", price[2].string + \"원입니다\")\n",
    "print(\"위엔화\", price[3].string + \"원입니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing /Users/bomilee/Documents/GitHub/python_basic/day_07/headline.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"/Users/bomilee/Documents/GitHub/python_basic/day_07/headline.py\"#소스저장\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "url = \"https://news.naver.com/\"\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "head = soup.select(\"#today_main_news > div.hdline_news > ul > li > div.hdline_article_tit > a\")\n",
    "\n",
    "for a in head:\n",
    "    print(a.string)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
